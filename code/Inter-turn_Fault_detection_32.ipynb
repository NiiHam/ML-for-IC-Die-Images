{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cupy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmemory_profiler\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcupy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mPIL\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cupy'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import memory_profiler\n",
    "import numpy\n",
    "import cupy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import scipy\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import sys\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from tensorflow.keras.layers import Conv3D,Activation,Conv2D, ConvLSTM2D, MaxPooling2D, MaxPooling3D, BatchNormalization, Flatten, Input, Dense, GRU, Embedding, LSTM, SimpleRNN, Dropout, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.optimizers import RMSprop,Adam,SGD,Adadelta,Adagrad,Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow import keras\n",
    "from matplotlib import style\n",
    "#style.use('classic')\n",
    "from joblib import Parallel, delayed\n",
    "from numpy import genfromtxt\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image \n",
    "from scipy.signal import resample\n",
    "from io import StringIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9271484484393346160\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4924650291\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 16299296181481872550\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "\n",
    "try:\n",
    "  IN_COLAB = True\n",
    "  from google.colab import drive\n",
    "  from tensorflow.keras.optimizers.legacy import RMSprop,Adam,SGD,Adadelta,Adagrad,Adamax\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  from tensorflow.keras.optimizers import RMSprop,Adam,SGD,Adadelta,Adagrad,Adamax\n",
    "\n",
    "from tensorflow.keras.optimizers.legacy import RMSprop,Adam,SGD,Adadelta,Adagrad,Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "from tensorflow.keras.utils import plot_model, to_categorical, normalize\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "\n",
    "try:\n",
    "  # tf.debugging.experimental.enable_dump_debug_info('.', tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n",
    "  # tf.debugging.set_log_device_placement(True)\n",
    "  from tensorflow.python.client import device_lib\n",
    "\n",
    "  device_name = tf.test.gpu_device_name()\n",
    "  if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "  print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "  config = tf.compat.v1.ConfigProto()\n",
    "  config.gpu_options.allow_growth = True\n",
    "  config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
    "  sess = tf.compat.v1.InteractiveSession(config=config)\n",
    "  set_session(sess)\n",
    "  print(device_lib.list_local_devices())\n",
    "  gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "  for gpu in gpus:\n",
    "    try:\n",
    "    \n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "      # Restrict TensorFlow to only use the first GPU\n",
    "      tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "      logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "      print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "      # Visible devices must be set before GPUs have been initialized\n",
    "      print(e)\n",
    "\n",
    "except Exception as error:\n",
    "    print(\"Error trying to configure computing device.\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.platform import build_info as tf_build_info\n",
    "print(\"Tensorflow verison: \",tf.__version__)\n",
    "print(\"CUDA verison: \", tf_build_info.cuda_version_number)\n",
    "print(\"CUDNN verison: \", tf_build_info.cudnn_version_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentageShort = np.arange(0.0001,1,1/400)*100\n",
    "percentageShort = np.around(percentageShort, decimals=2, out=None)\n",
    "pShort = numpy.concatenate((percentageShort, percentageShort, percentageShort, percentageShort, percentageShort, percentageShort), axis=0)\n",
    "\n",
    "default_path=r\"C:\\Users\\hp\\iCloudDrive\\Final Year Project\\Matlab\\Windings\"\n",
    "phase = ['phaseA', 'phaseB', 'phaseC']\n",
    "phaseBool = ['100','010','001']\n",
    "winding = ['Primary','Secondary']\n",
    "windingBool = ['10','01']\n",
    "faultBool = ['0','1']\n",
    "section = ['Va', 'Vb', 'Vc']\n",
    "parameter = ['Voltage','Current']\n",
    "name = ['output_voltage','output_current']\n",
    "ext = '.csv'\n",
    "im_ext = '.png'\n",
    "dataset1=[]\n",
    "dataset2=[]\n",
    "dataset3=[]\n",
    "dataset4=[]\n",
    "dataset5=[]\n",
    "dataset6=[]\n",
    "\n",
    "locationDataset = []\n",
    "magnitudeDataset = []\n",
    "imageDataset = []\n",
    "faultDataset = []\n",
    "\n",
    "count=1\n",
    "\n",
    "#final_path = os.path.join\n",
    "\n",
    "for percent in percentageShort:\n",
    "    count=1\n",
    "    percent=str(percent)\n",
    "    #print(percent)\n",
    "    for sect in section:\n",
    "        for k,winDing in enumerate(winding):\n",
    "            windingB = windingBool[k]\n",
    "            for j, pHase in enumerate(phase):\n",
    "                phaseB = phaseBool[j]\n",
    "                locationBool = [phaseB + windingB]\n",
    "                \n",
    "                if j==0:\n",
    "                    percentBool = np.array([float(percent),0,0])\n",
    "                elif j==1:\n",
    "                    percentBool = np.array([0,float(percent),0])\n",
    "                elif j==2:\n",
    "                    percentBool = np.array([0,0,float(percent)])\n",
    "                \n",
    "                for i,param in enumerate(parameter):\n",
    "                    new_path = os.path.join(default_path,pHase,winDing,sect,param,(name[i]+percent+ext))\n",
    "                    #print(new_path)\n",
    "                    \n",
    "                    if count<=6 and count>0:\n",
    "                        dat = [new_path,locationBool,percentBool,faultBool[1]]\n",
    "                        dataset1.append(dat)\n",
    "                    if count<=12 and count>6:\n",
    "                        dat = [new_path,locationBool,percentBool,faultBool[1]]\n",
    "                        dataset2.append(dat)\n",
    "                    if count<=18 and count>12:\n",
    "                        dat = [new_path,locationBool,percentBool,faultBool[1]]\n",
    "                        dataset3.append(dat)\n",
    "                    if count<=24 and count>18:\n",
    "                        dat = [new_path,locationBool,percentBool,faultBool[1]]\n",
    "                        dataset4.append(dat)\n",
    "                    if count<=30 and count>24:\n",
    "                        dat = [new_path,locationBool,percentBool,faultBool[1]]\n",
    "                        dataset5.append(dat)\n",
    "                        dat = [new_path,locationBool,percentBool,faultBool[1]]\n",
    "                    if count<=36 and count>30:\n",
    "                        dat = [new_path,locationBool,percentBool,faultBool[1]]\n",
    "                        dataset6.append(dat)\n",
    "                    \n",
    "                    count= count+1\n",
    "\n",
    "datasetPrime = dataset1 + dataset2 +dataset3 + dataset4 + dataset5 + dataset6\n",
    "i_count = np.arange(0,len(datasetPrime),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 sets of images will be generated.\n",
      "num_train: 640 num_val: 112 num_test: 48\n",
      "steps_per_epoch: 42\n",
      "train_validation_steps: 112 test_validation_steps: 48\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1             \n",
    "num_images = 6\n",
    "percentage_class = 1\n",
    "location_class = 3\n",
    "length = int((1200 * num_images))\n",
    "num_data = int(length/6)\n",
    "print(f'{num_data} sets of images will be generated.')\n",
    "train_split = 0.80\n",
    "channels = num_images * 3 #6 RGB images (3 channels)\n",
    "imageDataset = numpy.ndarray(shape=(288, 432, channels))\n",
    "num_train = int(train_split * num_data)\n",
    "num_val = int(0.7*(num_data - num_train))\n",
    "num_test = (num_data - num_train) - num_val\n",
    "steps_per_epoch = int((num_train/batch_size)/15)\n",
    "train_validation_steps = int(num_val/batch_size)\n",
    "test_validation_steps = int(num_test/batch_size)\n",
    "print('num_train:',num_train, 'num_val:',num_val, 'num_test:',num_test)\n",
    "print('steps_per_epoch:', steps_per_epoch)\n",
    "print('train_validation_steps:', train_validation_steps, 'test_validation_steps:', test_validation_steps)\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "range((800*6),(800*6)+(800*6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count=800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imageDataset shape:  (288, 432, 18)\n",
      "short_data size:  (2400,)\n",
      "2400 sets of images prepared\n",
      "x_train shape:  (640, 288, 432, 18)\n",
      "y_train[0] shape:  (640,)\n",
      "y_train[1] shape:  (640,)\n",
      "x_test shape:  (1760, 288, 432, 18)\n",
      "y_test[0] shape:  (1760,)\n",
      "y_test[1] shape:  (1760,)\n"
     ]
    }
   ],
   "source": [
    "short_dat=[]\n",
    "im_data = []\n",
    "image_dat=[]\n",
    "loc_data =[]  \n",
    "    \n",
    "for i in range(length):\n",
    "        #print(i)\n",
    "        if int(i)%6 == 0:\n",
    "            short = pShort[count]\n",
    "            #print('short',short)\n",
    "            count+=1\n",
    "        lenShort = len(str(short))    \n",
    "        if lenShort<=4:\n",
    "            image_path = (datasetPrime[i][0])[:-9]\n",
    "            param_type = (datasetPrime[i][0])[-9]\n",
    "            short_percent = (datasetPrime[i][0])[-8:-4]\n",
    "            #print(short_percent, image_path)\n",
    "            #break\n",
    "        else:\n",
    "            image_path = (datasetPrime[i][0])[:-9-1]\n",
    "            param_type = (datasetPrime[i][0])[-9-1]\n",
    "            short_percent = (datasetPrime[i][0])[-9:-4]\n",
    "            #print(short_percent, image_path)\n",
    "            #break      \n",
    "        #print(str(image_path + (f'*{param_type}{short}*.png')))    \n",
    "        files=glob.glob(str(image_path + (f'*{param_type}{short}*.png')))\n",
    "        flen = len(files)\n",
    "        \n",
    "        if flen != 3:\n",
    "            for file in files:\n",
    "                #os.remove(file)\n",
    "                raise Exception(f'Wrong file count: Expected 3 files but got {flen} {file}. Please rectify')\n",
    "           \n",
    "\n",
    "            \n",
    "        if i%2 is 1:\n",
    "            for myFile in files:\n",
    "                #print(myFile)\n",
    "                image=Image.open(myFile)\n",
    "                image = cv2.imread (myFile)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                image = image.reshape(image.shape[0],image.shape[1],image.shape[2])\n",
    "                #plt.imshow(image)\n",
    "                #plt.show()\n",
    "                if image.shape != (288, 432, 3):\n",
    "                    print(myFile)\n",
    "                    plt.imshow(image)\n",
    "                    plt.show()\n",
    "                    print(image.shape)\n",
    "                    #os.remove(myFile)\n",
    "                    prepData()\n",
    "                    break\n",
    "                im_data.append(np.asnumpy(image))\n",
    "                proceed = True\n",
    "            #print(count)\n",
    "    \n",
    "        else:\n",
    "            for myFile in files:\n",
    "                #print(myFile)\n",
    "                image=Image.open(myFile)\n",
    "                image = cv2.imread (myFile)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                image = image.reshape(image.shape[0],image.shape[1],image.shape[2])\n",
    "                #plt.imshow(image)\n",
    "                #plt.show()\n",
    "                if image.shape != (288, 432, 3):\n",
    "                    print(myFile)\n",
    "                    plt.imshow(image)\n",
    "                    plt.show()\n",
    "                    print(image.shape)\n",
    "                    #os.remove(myFile)\n",
    "                    prepData()\n",
    "                    break\n",
    "                im_data.append (np.asnumpy(image))\n",
    "                proceed = False\n",
    "            \n",
    "        if proceed:\n",
    " \n",
    "            imageDataset = numpy.concatenate((im_data[0], im_data[1],\n",
    "                                              im_data[2], im_data[3],\n",
    "                                              im_data[4], im_data[5]), axis=2)\n",
    "            image_dat.append(imageDataset)\n",
    "            value = (datasetPrime[i][1][0])\n",
    "            if value=='10010' or value=='10001':\n",
    "               value = 0\n",
    "            elif value=='01010' or value=='01001':\n",
    "                value = 1\n",
    "            elif value=='00110' or value=='00101' :\n",
    "                value = 2\n",
    "            \n",
    "            loc_data.append(value)\n",
    "            #print('image_data shape:', np.asnumpy(image_dat).shape)           \n",
    "            #print('im_data shape:', np.array(im_data).shape)\n",
    "            short_dat.append(short)\n",
    "            #print('short',short)\n",
    "            #print('count',count)\n",
    "            im_data = []\n",
    "\n",
    "short_data = np.asnumpy(short_dat)/100.         \n",
    "image_dat = np.asnumpy(image_dat)/255.\n",
    "y_data=[]\n",
    "for i in range(len(short_data)):\n",
    "    y_data.append([short_data[i],loc_data[i]])\n",
    "print('imageDataset shape: ', imageDataset.shape)\n",
    "print('short_data size: ', short_data.shape)\n",
    "print(len(image_dat), 'sets of images prepared')\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(image_dat, y_data, train_size=train_split, random_state=50)\n",
    "x_train=image_dat[:num_train]\n",
    "x_test=image_dat[num_train:]\n",
    "y_train=y_data[:num_train]\n",
    "y_test=y_data[num_train:]\n",
    "\n",
    "\n",
    "del image_dat\n",
    "gc.collect()\n",
    "y_train = np.asnumpy(y_train)\n",
    "y_test = np.asnumpy(y_test)\n",
    "\n",
    "print('x_train shape: ', x_train.shape) \n",
    "print('y_train[0] shape: ', y_train[:,0].shape)\n",
    "print('y_train[1] shape: ', y_train[:,1].shape)    \n",
    "print('x_test shape: ', x_test.shape)  \n",
    "print('y_test[0] shape: ', y_test[:,0].shape)\n",
    "print('y_test[1] shape: ', y_test[:,1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%store -r x_train\n",
    "%store -r x_test\n",
    "%store -r y_train\n",
    "%store -r y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(samples, time, rows, cols, channels)\n",
    "(None , 1, 360, 1080, 3) means that you have only one sample that is a sequence of 1 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(batch_size, train=None, validation=None):\n",
    "    \"\"\"\n",
    "    Generator function for creating random batches of training-data.\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        num_samples = num_train\n",
    "        x_samples = x_train\n",
    "        y_samples = y_train\n",
    "        print('using train samples')\n",
    "    elif validation:\n",
    "        num_samples = num_val\n",
    "        x_samples = x_test[:num_samples]\n",
    "        y_samples = y_test[:num_samples]\n",
    "        print('using validation samples')\n",
    "    else:\n",
    "        num_samples = num_test\n",
    "        x_samples = x_test[-num_samples:]\n",
    "        y_samples = y_test[-num_samples:]\n",
    "        print('using test samples')\n",
    "    # Infinite loop.\n",
    "    while True:\n",
    "        # Allocate a new array for the batch of input-signals.\n",
    "        x_shape = (batch_size, x_samples.shape[1],x_samples.shape[2],x_samples.shape[3])\n",
    "        x_batch = numpy.empty(shape=x_shape,)\n",
    "        #print(x_shape)\n",
    "        \n",
    "        # Allocate a new array for the batch of output-signals.\n",
    "        y1_shape = (batch_size, percentage_class)\n",
    "        y1_batch = numpy.empty(shape=y1_shape,)\n",
    "        y2_shape = (batch_size, 1)\n",
    "        y2_batch = numpy.empty(shape=y2_shape,)\n",
    "            \n",
    "        # Fill the batch with random sequences of data.\n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            # Get a random start-index.\n",
    "            # This points somewhere into the training-data.\n",
    "            idx = int(np.random.randint(num_samples - 1))\n",
    "            \n",
    "            # Copy the sequences of data starting at this index.\n",
    "            x_batch[i] = x_samples[idx]\n",
    "\n",
    "            y1_batch[i] = np.asnumpy(y_samples[:,0][idx])\n",
    "            y2_batch[i] = np.asnumpy(y_samples[:,1][idx])\n",
    "            \n",
    "\n",
    "        x_batch=x_batch.reshape(batch_size, x_samples.shape[1],x_samples.shape[2],x_samples.shape[3])\n",
    "        y_batch=[y1_batch, to_categorical(y2_batch, num_classes=location_class, dtype='float32')]\n",
    "        \n",
    "        #print(y_batch)\n",
    "        yield (x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using train samples\n",
      "x_train shape:  (1, 288, 432, 18) x_train dtype: float64\n",
      "y_train[0] shape:  (1, 1) y_train[0] dtype: float64\n",
      "y_train[1] shape:  (1, 3) y_train[1] dtype: float32\n"
     ]
    }
   ],
   "source": [
    "train_generator = batch_generator(batch_size=batch_size, train=True, validation=True)\n",
    "x_train_batch, y_train_batch=next(train_generator)\n",
    "\n",
    "print('x_train shape: ', x_train_batch.shape, 'x_train dtype:', x_train_batch.dtype)  \n",
    "print('y_train[0] shape: ', y_train_batch[0].shape, 'y_train[0] dtype:', y_train_batch[0].dtype)\n",
    "print('y_train[1] shape: ', y_train_batch[1].shape, 'y_train[1] dtype:', y_train_batch[1].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using validation samples\n",
      "x_val shape:  (1, 288, 432, 18) x_val dtype: float64\n",
      "y_val[0] shape:  (1, 1) y_val[] dtype: float64\n",
      "y_val[1] shape:  (1, 3) y_val[] dtype: float32\n"
     ]
    }
   ],
   "source": [
    "val_generator = batch_generator(batch_size=batch_size, train=False, validation=True)\n",
    "x_val_batch, y_val_batch=next(val_generator)\n",
    "\n",
    "print('x_val shape: ', x_val_batch.shape, 'x_val dtype:', x_val_batch.dtype)  \n",
    "print('y_val[0] shape: ', y_val_batch[0].shape, 'y_val[] dtype:', y_val_batch[0].dtype)\n",
    "print('y_val[1] shape: ', y_val_batch[1].shape, 'y_val[] dtype:', y_val_batch[1].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using test samples\n",
      "x_test shape:  (1, 288, 432, 18) x_test dtype: float64\n",
      "y_test[0] shape:  (1, 1) y_test dtype: float64\n",
      "y_test[1] shape:  (1, 3) y_test dtype: float32\n"
     ]
    }
   ],
   "source": [
    "test_generator = batch_generator(batch_size=batch_size, train=False, validation=False)\n",
    "x_test_batch, y_test_batch=next(test_generator)\n",
    "\n",
    "print('x_test shape: ', x_test_batch.shape, 'x_test dtype:', x_test_batch.dtype)  \n",
    "print('y_test[0] shape: ', y_test_batch[0].shape, 'y_test dtype:', y_test_batch[0].dtype)\n",
    "print('y_test[1] shape: ', y_test_batch[1].shape, 'y_test dtype:', y_test_batch[1].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Prepare Validation data\n",
    "\n",
    "x_val_shape = (len(x_test), sequence_length, x_train.shape[1], x_train.shape[2], x_train.shape[3])\n",
    "x_val = numpy.empty(shape=x_val_shape)\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    x_val[i] = x_test[i]\n",
    "\n",
    "x_val = x_val.reshape(len(x_test), sequence_length, x_train.shape[1], x_train.shape[2],x_train.shape[3])\n",
    "\n",
    "validation_data = (x_val[:batch_size], y_test[:batch_size])\n",
    " \n",
    "print('x_val shape: ', validation_data[0].shape)  \n",
    "print('y_val shape: ', validation_data[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 432, 18)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = RMSprop(lr=1e-5)\n",
    "momentum=0.25\n",
    "datshape = (x_train.shape[1], x_train.shape[2], x_train.shape[3])\n",
    "datshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "del model\n",
    "model = Sequential()\n",
    "\n",
    "#First Convolutional layer\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), input_shape = datshape, padding='same', activation='selu', data_format='channels_last'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.05))\n",
    "\n",
    "#Second Convolutional layer\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.05))\n",
    "\n",
    "#Third Convolutional layer\n",
    "model.add(Conv2D(filters = 128, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.05))\n",
    "\n",
    "#Fourth Convolutional layer\n",
    "model.add(Conv2D(filters = 128, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.05))\n",
    "\n",
    "#Fifth Convolutional layer\n",
    "model.add(Conv2D(filters = 256, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "#Sixth Convolutional layer\n",
    "model.add(Conv2D(filters = 256, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "#Seventh Convolutional layer\n",
    "model.add(Conv2D(filters = 256, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "#Eighth Convolutional layer\n",
    "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2), strides=(2,2)))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "#Flattening\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "#Hidden Layer\n",
    "#model.add(Dense(units = 1024, activation='selu')) \n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "#Hidden Layer\n",
    "model.add(Dense(units=4096, activation='selu'))\n",
    "model.add(Dropout(0.25))\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "#Hidden Layer\n",
    "model.add(Dense(units=4096, activation='selu'))\n",
    "model.add(Dropout(0.25))\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "#Output Layer\n",
    "model.add(Dense(percentage_class, activation='selu'))\n",
    "\n",
    "\n",
    "model.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Inter-turn_Fault_Detection_AI_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "3-phase_Scaleograms_input (Inpu [(None, 288, 432, 18 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 288, 432, 72) 11736       3-phase_Scaleograms_input[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 144, 216, 72) 0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 144, 216, 72) 46728       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 72, 108, 72)  0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 72, 108, 72)  46728       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 36, 54, 72)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 36, 54, 72)   46728       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 18, 27, 72)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 18, 27, 72)   46728       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 9, 13, 72)    0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 9, 13, 72)    46728       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 4, 6, 72)     0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 4, 6, 72)     46728       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 2, 3, 72)     0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 2, 3, 72)     46728       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 1, 1, 72)     0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 72)           0           max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2048)         149504      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          9344        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2048)         4196352     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          16512       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Magnitude (Dense)               (None, 1)            2049        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Location (Dense)                (None, 3)            387         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,712,980\n",
      "Trainable params: 4,712,980\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "#del model\n",
    "\n",
    "def create_convnet(img_path='Interturn_AI_model_image.png'):\n",
    "  \n",
    "    image_input = Input(shape = datshape, name=\"3-phase_Scaleograms_input\")\n",
    "    \n",
    "    first_Conv2D = Conv2D(filters = 64, kernel_size = (3,3), input_shape = datshape, padding='same', activation='selu', data_format='channels_last')(image_input)\n",
    "    first_Pooling = MaxPooling2D(pool_size = (2,2))(first_Conv2D)\n",
    "    \n",
    "    second_Conv2D = Conv2D(filters = 64, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last')(first_Pooling)\n",
    "    second_Pooling = MaxPooling2D(pool_size = (2,2))(second_Conv2D)\n",
    "    \n",
    "    third_Conv2D = Conv2D(filters = 82, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last')(second_Pooling)\n",
    "    third_Pooling = MaxPooling2D(pool_size = (2,2))(third_Conv2D)\n",
    "    \n",
    "    forth_Conv2D = Conv2D(filters = 82, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last')(third_Pooling)\n",
    "    forth_Pooling = MaxPooling2D(pool_size = (2,2))(forth_Conv2D)\n",
    "    \n",
    "    fifth_Conv2D = Conv2D(filters = 128, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last')(forth_Pooling)\n",
    "    fifth_Pooling = MaxPooling2D(pool_size = (2,2))(fifth_Conv2D)\n",
    "    \n",
    "    sixth_Conv2D = Conv2D(filters = 128, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last')(fifth_Pooling)\n",
    "    sixth_Pooling = MaxPooling2D(pool_size = (2,2))(sixth_Conv2D)\n",
    "    \n",
    "    seventh_Conv2D = Conv2D(filters = 256, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last')(sixth_Pooling)\n",
    "    seventh_Pooling = MaxPooling2D(pool_size = (2,2))(seventh_Conv2D)\n",
    "    \n",
    "    eighth_Conv2D = Conv2D(filters = 512, kernel_size = (3,3), padding='same', activation='selu', data_format='channels_last')(seventh_Pooling)\n",
    "    eighth_Pooling = MaxPooling2D(pool_size = (2,2), strides=(2,2))(eighth_Conv2D)\n",
    "      \n",
    "    flattened_layer = Flatten()(eighth_Pooling)\n",
    "    \n",
    "    first_DNN_0 =  Dense(units=4096, activation='selu')(flattened_layer)\n",
    "    first_Dropout_0 = Dropout(0.25)(first_DNN_0)\n",
    "\n",
    "    second_DNN_0 =  Dense(units=4096, activation='selu')(first_Dropout_0)\n",
    "    second_Dropout_0 = Dropout(0.25)(second_DNN_0)\n",
    "\n",
    "    first_DNN_1 =  Dense(units=256, activation='selu')(second_Dropout_0)\n",
    "    first_Dropout_1 = Dropout(0.25)(first_DNN_1)\n",
    "\n",
    "    second_DNN_1 =  Dense(units=256, activation='selu')(first_Dropout_1)\n",
    "    second_Dropout_1 = Dropout(0.25)(second_DNN_1)\n",
    "    out_1 =  Dense(percentage_class, activation='selu', name='Magnitude')(second_Dropout_0)\n",
    "    \n",
    "    \n",
    "    out_2 =  Dense(location_class, activation='sigmoid', name='Location')(second_Dropout_1)\n",
    "\n",
    "        \n",
    "    model = Model(inputs=image_input, outputs=[out_1, out_2], name='Inter-turn_Fault_Detection_AI_model')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model = create_convnet()\n",
    "\n",
    "model.compile(loss=['mean_squared_error','categorical_crossentropy'], optimizer=optimizer,)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback Functions\n",
    "\n",
    "During training we want to save checkpoints and log the progress to TensorBoard so we create the appropriate callbacks for Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is the callback for writing checkpoints during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = r'C:\\Users\\hp\\iCloudDrive\\Final Year Project\\Python Stuff\\AI model\\Model_checkpoint\\Inter-turn_fault_detect_model_checkpoint_6.keras'\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=1,\n",
    "                                      save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the callback for stopping the optimization when performance worsens on the validation-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the callback for writing the TensorBoard log during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[WinError 5] Access is denied: 'E:\\\\Dropbox\\\\AI\\\\logs\\\\run3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-599c2eeb59ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mAddress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'E:\\Dropbox\\AI'\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAddress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAddress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mNAME\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'run'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfolderCount\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mlogdir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'logs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 5] Access is denied: 'E:\\\\Dropbox\\\\AI\\\\logs\\\\run3'"
     ]
    }
   ],
   "source": [
    "folderNumber=4\n",
    "folderCount = str(4)  \n",
    "#folderCount=str(input(\"Enter the Session RUN number: \"))\n",
    "NAME = 'run'+ folderCount\n",
    "logdir = os.path.join(r'logs', NAME)\n",
    "Address=str(os.path.join(r'E:\\Dropbox\\AI' ,logdir))\n",
    "#if os.path.exists(Address):\n",
    "    #os.remove(Address)\n",
    "    #NAME = 'run'+ (folderCount+1)\n",
    "    #logdir = os.path.join(r'logs', NAME)\n",
    "    #Address=str(os.path.join(r'E:\\Dropbox\\AI' ,logdir))\n",
    "    #if os.path.exists(Address):\n",
    "    #raise Exception('folder exists')\n",
    "\n",
    "print(Address)\n",
    "callback_tensorboard = TensorBoard(log_dir=Address,\n",
    "                                   histogram_freq=0,\n",
    "                                   write_graph=True,\n",
    "                                   write_images=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This callback reduces the learning-rate for the optimizer if the validation-loss has not improved since the last epoch (as indicated by `patience=0`). The learning-rate will be reduced by multiplying it with the given factor. We set a start learning-rate of 1e-3 above, so multiplying it by 0.1 gives a learning-rate of 1e-4. We don't want the learning-rate to go any lower than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1,\n",
    "                                       min_lr=1e-6,\n",
    "                                       patience=0,\n",
    "                                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'callback_tensorboard' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-a55771835e3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m callbacks = [callback_early_stopping,\n\u001b[0;32m      2\u001b[0m              \u001b[0mcallback_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m              \u001b[0mcallback_tensorboard\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m              callback_reduce_lr]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'callback_tensorboard' is not defined"
     ]
    }
   ],
   "source": [
    "callbacks = [callback_early_stopping,\n",
    "             callback_checkpoint,\n",
    "             callback_tensorboard,\n",
    "             callback_reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'C:\\Users\\hp\\iCloudDrive\\Final Year Project\\Python Stuff\\AI model\\Model_architecture\\Inter-turn_Fault_Detection_AI_model_6'\n",
    "def train_model(resume, epochs, initial_epoch, batch_size,model):\n",
    "    def fit_model():\n",
    "        \n",
    "        print(model.summary())\n",
    "        history=model.fit(    train_generator, \n",
    "                              steps_per_epoch=steps_per_epoch, \n",
    "                              epochs=EPOCHS, \n",
    "                              verbose=1, \n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=val_generator, \n",
    "                              validation_steps=train_validation_steps, \n",
    "                              #validation_freq=1,\n",
    "                              #class_weight=None, \n",
    "                              #max_queue_size=10, \n",
    "                              #workers=8, \n",
    "                              #use_multiprocessing=False,\n",
    "                              shuffle=True,) \n",
    "                              #initial_epoch=initial_epoch)\n",
    "        model.load_weights(path_checkpoint)            \n",
    "        model.save(filepath)\n",
    "        model.evaluate(test_generator, steps=test_validation_steps)\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    if resume:\n",
    "        try:\n",
    "            #del model\n",
    "            #model = load_model(filepath)\n",
    "            model.load_weights(path_checkpoint)\n",
    "            print(model.summary())\n",
    "            print(\"Model loading....\")\n",
    "            model.evaluate(test_generator, steps=test_validation_steps)\n",
    "            \n",
    "        except Exception as error:\n",
    "            print(\"Error trying to load checkpoint.\")\n",
    "            print(error)\n",
    "\n",
    "            \n",
    "    def plot_train_history(history, title):\n",
    "        loss = history.history['loss']\n",
    "        val_loss = np.asnumpy(history.history['val_loss'])\n",
    "        accuracy = 1-np.asnumpy(loss)\n",
    "        val_accuracy = 1-np.asnumpy(val_loss)\n",
    "        epochs = range(len(loss))\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.plot(epochs, loss, label='training_loss') \n",
    "        plt.plot(epochs, val_loss, label='validation_loss')\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.plot(epochs, accuracy, label='training_accuracy') \n",
    "        plt.plot(epochs, val_accuracy, label='validation_accuracy')\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    # Training the Model\n",
    "    history = fit_model()\n",
    "    plot_train_history(history, 'Model Training History ')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Inter-turn_Fault_Detection_AI_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "3-phase_Scaleograms_input (Inpu [(None, 288, 432, 18 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 288, 432, 72) 11736       3-phase_Scaleograms_input[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 144, 216, 72) 0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 144, 216, 72) 46728       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 72, 108, 72)  0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 72, 108, 72)  46728       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 36, 54, 72)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 36, 54, 72)   46728       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 18, 27, 72)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 18, 27, 72)   46728       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 9, 13, 72)    0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 9, 13, 72)    46728       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 4, 6, 72)     0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 4, 6, 72)     46728       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 2, 3, 72)     0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 2, 3, 72)     46728       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 1, 1, 72)     0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 72)           0           max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2048)         149504      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          9344        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2048)         4196352     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          16512       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Magnitude (Dense)               (None, 1)            2049        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Location (Dense)                (None, 3)            387         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,712,980\n",
      "Trainable params: 4,712,980\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'callbacks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-86b9537614c3>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(resume, epochs, initial_epoch, batch_size, model)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# Training the Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m     \u001b[0mplot_train_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Model Training History '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-86b9537614c3>\u001b[0m in \u001b[0;36mfit_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                               \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                               \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_validation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'callbacks' is not defined"
     ]
    }
   ],
   "source": [
    "EPOCHS=1000\n",
    "#steps_per_epoch = int((num_train/batch_size)/17)\n",
    "train_model(resume=False, epochs=EPOCHS, initial_epoch=0, batch_size=batch_size, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 31s 276ms/step - loss: 1.0941 - Magnitude_loss: 0.0261 - Location_loss: 1.0680\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 1.4826 - Magnitude_loss: 0.4121 - Location_loss: 1.0705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.482599916557471, 0.4121133, 1.0704867]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(path_checkpoint)\n",
    "model.evaluate(train_generator, steps=train_validation_steps)\n",
    "model.evaluate(test_generator, steps=test_validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": " Dst tensor is not initialized.\n\t [[{{node IteratorGetNext/_2}}]] [Op:__inference_distributed_function_4912]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-93e2bb163362>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m   1014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m               \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m               total_epochs=1)\n\u001b[0m\u001b[0;32m    476\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    636\u001b[0m               *args, **kwds)\n\u001b[0;32m    637\u001b[0m       \u001b[1;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_concrete_stateful_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minner_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mE:\\Program Files\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m:  Dst tensor is not initialized.\n\t [[{{node IteratorGetNext/_2}}]] [Op:__inference_distributed_function_4912]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "y_pred = pd.DataFrame(model.predict(x_test))\n",
    "y_pred = np.asnumpy(y_pred).reshape(-1,2)\n",
    "y_pred = [np.asnumpy(list(y_pred[:,0])).reshape(-1,1) , np.asnumpy(list(y_pred[:,1])).reshape(-1,3)]\n",
    "y_pred[1] = y_pred[1].argmax(axis=-1)\n",
    "y_pred = [y_pred[0].reshape(len(y_pred[0]),1), y_pred[1].reshape(len(y_pred[1]),1)]\n",
    "\n",
    "y_pred_shape = (len(y_pred[1]), 1)\n",
    "y_pred_0 = numpy.empty(shape=y_pred_shape,)\n",
    "y_pred_1 = numpy.empty(shape=y_pred_shape, dtype='int32')\n",
    "            \n",
    "# Fill the batch with random sequences of data.\n",
    "for i in range(len(y_pred[1])):\n",
    "    y_pred_0[i] = np.asnumpy(y_pred[0][i])\n",
    "    y_pred_1[i] = np.asnumpy(y_pred[1][i])\n",
    "\n",
    "Y_pred = numpy.concatenate([y_pred_0, y_pred_1], axis=1)\n",
    "Y_pred.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1760, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = y_test\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-bd980a2f34e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mplot_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-bd980a2f34e3>\u001b[0m in \u001b[0;36mplot_prediction\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'o'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'True values'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'o'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Predicted values'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "#style.use('classic')\n",
    "def plot_prediction():\n",
    "        plt.figure(figsize=(30,10))\n",
    "        plt.plot( y_true[:,0]*100, 'o', label='True values') \n",
    "        plt.plot( Y_pred[:,0]*100, 'o',label='Predicted values')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(30,10))\n",
    "        plt.plot( y_true[:,1],  label='True values') \n",
    "        plt.plot( Y_pred[:,1],  label='Predicted values')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "plot_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
